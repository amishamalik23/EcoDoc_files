{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc62751944af483180db26fd91bee0cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5d2a524b16646eab1152cbeade867b7",
              "IPY_MODEL_f61e2d13548745159609cbbccce24795",
              "IPY_MODEL_d981945257f74ab6a4ff8a3392e69fb6"
            ],
            "layout": "IPY_MODEL_a39da6d399874e7f80682fdfebbe5caa"
          }
        },
        "a5d2a524b16646eab1152cbeade867b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddca93754a044ca5be344ba9e5ba6e0b",
            "placeholder": "​",
            "style": "IPY_MODEL_0bf1092fcec4453b99a51801c04e98d4",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f61e2d13548745159609cbbccce24795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_463838c29f234a8e86dce89d95436c30",
            "max": 727,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ea3b9f9bca2457190f2083f6fee27c8",
            "value": 727
          }
        },
        "d981945257f74ab6a4ff8a3392e69fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64466599b1d542e69e61321d5fa55fdb",
            "placeholder": "​",
            "style": "IPY_MODEL_65717a2bd44d498bb1fc44af10b1f73d",
            "value": " 727/727 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "a39da6d399874e7f80682fdfebbe5caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddca93754a044ca5be344ba9e5ba6e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf1092fcec4453b99a51801c04e98d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "463838c29f234a8e86dce89d95436c30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea3b9f9bca2457190f2083f6fee27c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64466599b1d542e69e61321d5fa55fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65717a2bd44d498bb1fc44af10b1f73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a297524ae364f2b808392e0a183baa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e84b730b4bd479faba620179b5c1e08",
              "IPY_MODEL_14538039b8bd479ebc6bf00d1b45daf8",
              "IPY_MODEL_6ae1d13e53c443d79c51f9f2521fcbab"
            ],
            "layout": "IPY_MODEL_c25a73fd05534780a91474fd3331a3c7"
          }
        },
        "6e84b730b4bd479faba620179b5c1e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0774f00841b14438ac51a13acf3c6bbc",
            "placeholder": "​",
            "style": "IPY_MODEL_15021e72b8f447b5ad7c7f3721b6a6be",
            "value": "vocab.json: 100%"
          }
        },
        "14538039b8bd479ebc6bf00d1b45daf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62c3c3ada016494b834778fd9e79fdd9",
            "max": 898669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbfc0b38f20048a6861d8902b08bef33",
            "value": 898669
          }
        },
        "6ae1d13e53c443d79c51f9f2521fcbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07a2f6dc5b344f8ab431b4ce57fb9d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_dddea27043aa4364920bb248e89f62c3",
            "value": " 899k/899k [00:00&lt;00:00, 19.3MB/s]"
          }
        },
        "c25a73fd05534780a91474fd3331a3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0774f00841b14438ac51a13acf3c6bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15021e72b8f447b5ad7c7f3721b6a6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62c3c3ada016494b834778fd9e79fdd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbfc0b38f20048a6861d8902b08bef33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07a2f6dc5b344f8ab431b4ce57fb9d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dddea27043aa4364920bb248e89f62c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14a0d540008a4b0cb2a83c170d04763e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e85729e232a740b8ba74fb873d8c65cb",
              "IPY_MODEL_e7035bd26da543608daeeac82671687b",
              "IPY_MODEL_558239a567024b5fb3967d66e93bc618"
            ],
            "layout": "IPY_MODEL_7ed1593998e14ed1be7a6229200d5a75"
          }
        },
        "e85729e232a740b8ba74fb873d8c65cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52042a64dced4d8895346651d0b7f671",
            "placeholder": "​",
            "style": "IPY_MODEL_827d543ae0be4316bee9e54fb38a462f",
            "value": "merges.txt: 100%"
          }
        },
        "e7035bd26da543608daeeac82671687b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c954bb50db444b4c8536da694310021a",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f0186f00e254cbcbb160abf271f073c",
            "value": 456318
          }
        },
        "558239a567024b5fb3967d66e93bc618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d5a66494be4a9bbe6b586a03918be7",
            "placeholder": "​",
            "style": "IPY_MODEL_ae7865408e6847aabd64217ceb726799",
            "value": " 456k/456k [00:00&lt;00:00, 13.4MB/s]"
          }
        },
        "7ed1593998e14ed1be7a6229200d5a75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52042a64dced4d8895346651d0b7f671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827d543ae0be4316bee9e54fb38a462f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c954bb50db444b4c8536da694310021a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0186f00e254cbcbb160abf271f073c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66d5a66494be4a9bbe6b586a03918be7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae7865408e6847aabd64217ceb726799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b721cc7c167a4404a46ca02d211b8a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cdb40c641eb40d881d838fa45e59e6a",
              "IPY_MODEL_7ee406e9c1544faa9a415d1e328497cb",
              "IPY_MODEL_239b75bbcec9427fb7a172c7663f5124"
            ],
            "layout": "IPY_MODEL_5319fa75e0454e77bbd04aaa4fadd57e"
          }
        },
        "3cdb40c641eb40d881d838fa45e59e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b74d89b020e40ffb03354e0340a4bef",
            "placeholder": "​",
            "style": "IPY_MODEL_489a2051dbc94b47a89a14fd0d68c30c",
            "value": "tokenizer.json: 100%"
          }
        },
        "7ee406e9c1544faa9a415d1e328497cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7681586e480468999db79557ce93832",
            "max": 2107652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93c044674a5744fbb6de37092221510d",
            "value": 2107652
          }
        },
        "239b75bbcec9427fb7a172c7663f5124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73f602c68f384c828325f98be21aa251",
            "placeholder": "​",
            "style": "IPY_MODEL_44a5a3176e48464b9e6f081c1a18e4de",
            "value": " 2.11M/2.11M [00:00&lt;00:00, 47.5MB/s]"
          }
        },
        "5319fa75e0454e77bbd04aaa4fadd57e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b74d89b020e40ffb03354e0340a4bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489a2051dbc94b47a89a14fd0d68c30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7681586e480468999db79557ce93832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c044674a5744fbb6de37092221510d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73f602c68f384c828325f98be21aa251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a5a3176e48464b9e6f081c1a18e4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4774ae8e5ff44e6ca70eb99a94e47581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3c77dfa1bd34deb9e9f9dc4e48e20c0",
              "IPY_MODEL_88d6b1bf4a644c3886964517de6ef623",
              "IPY_MODEL_3bf1afb7df3346938f820b38af52c6f2"
            ],
            "layout": "IPY_MODEL_e970050d95244554b7224f960134cd89"
          }
        },
        "c3c77dfa1bd34deb9e9f9dc4e48e20c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c70dad6c9a488ebf102bb84fe93c05",
            "placeholder": "​",
            "style": "IPY_MODEL_f3291db1cf4848abb982a72897f83cfc",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "88d6b1bf4a644c3886964517de6ef623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d387e0751c5447891bc686f5a40508b",
            "max": 357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_878413f7d065459c9472d5fdbf34e7e5",
            "value": 357
          }
        },
        "3bf1afb7df3346938f820b38af52c6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23cd18f72c194dcc8b9301f5bab81479",
            "placeholder": "​",
            "style": "IPY_MODEL_8a73e089167049748a7567fda1798aa5",
            "value": " 357/357 [00:00&lt;00:00, 11.5kB/s]"
          }
        },
        "e970050d95244554b7224f960134cd89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c70dad6c9a488ebf102bb84fe93c05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3291db1cf4848abb982a72897f83cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d387e0751c5447891bc686f5a40508b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878413f7d065459c9472d5fdbf34e7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23cd18f72c194dcc8b9301f5bab81479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a73e089167049748a7567fda1798aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "831c727e8b7548b895b25c2998d4de3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9086bbd4c6334540aa43ba93a74a8d60",
              "IPY_MODEL_e1b0e6b0d29a47d5879f158e4361e74b",
              "IPY_MODEL_6085bc5ce4344e048649e2a768e12395"
            ],
            "layout": "IPY_MODEL_8c1206d775e74a4f94bb13272055f0d0"
          }
        },
        "9086bbd4c6334540aa43ba93a74a8d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6454aff3d9048a285a0a58476066c3c",
            "placeholder": "​",
            "style": "IPY_MODEL_929ec3ef185e4443b86ef3937b30413c",
            "value": "config.json: 100%"
          }
        },
        "e1b0e6b0d29a47d5879f158e4361e74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6916530012243299f26698a7ffd7ec0",
            "max": 1007,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d08d74cbce444abda419d72f16376e10",
            "value": 1007
          }
        },
        "6085bc5ce4344e048649e2a768e12395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d69b6a586cf741808b515c637a5debaf",
            "placeholder": "​",
            "style": "IPY_MODEL_87b9ab37968a4a2b8351f0324170e8cf",
            "value": " 1.01k/1.01k [00:00&lt;00:00, 33.5kB/s]"
          }
        },
        "8c1206d775e74a4f94bb13272055f0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6454aff3d9048a285a0a58476066c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929ec3ef185e4443b86ef3937b30413c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6916530012243299f26698a7ffd7ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d08d74cbce444abda419d72f16376e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d69b6a586cf741808b515c637a5debaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b9ab37968a4a2b8351f0324170e8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81e42e1da1fb4b3aa52a9ef0b089a786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2eaa79537054cf393fe9f6349e36ee7",
              "IPY_MODEL_0d07f0376846415f809e023cb2ecd616",
              "IPY_MODEL_857f8f82c6db4db7857b0f7973f2825a"
            ],
            "layout": "IPY_MODEL_2358a33ca1cf4c7ca169eca88794807a"
          }
        },
        "d2eaa79537054cf393fe9f6349e36ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31b7d97211e849fe975685d202ea5d8c",
            "placeholder": "​",
            "style": "IPY_MODEL_3689978b85604049b6b33f817d5b54cc",
            "value": "model.safetensors: 100%"
          }
        },
        "0d07f0376846415f809e023cb2ecd616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_104e50c49b764a879efd7a2925991df6",
            "max": 525979192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6ac1f8f67ea4357ac1e23c5caabf79b",
            "value": 525979192
          }
        },
        "857f8f82c6db4db7857b0f7973f2825a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403bbce82cc14338a46688d04f2b09ea",
            "placeholder": "​",
            "style": "IPY_MODEL_da128c30e45e4507943d59dc57aa2248",
            "value": " 526M/526M [00:04&lt;00:00, 108MB/s]"
          }
        },
        "2358a33ca1cf4c7ca169eca88794807a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b7d97211e849fe975685d202ea5d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3689978b85604049b6b33f817d5b54cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "104e50c49b764a879efd7a2925991df6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ac1f8f67ea4357ac1e23c5caabf79b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "403bbce82cc14338a46688d04f2b09ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da128c30e45e4507943d59dc57aa2248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74fd9fd0083b43ffb7aede27d512c11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0098a541c98f4ed1abc17836d1669e9a",
              "IPY_MODEL_2067c584e4e1437bb02772796c6210cf",
              "IPY_MODEL_4742fe0813114583aa036c479959c0b9"
            ],
            "layout": "IPY_MODEL_c0d41eeff73e4ec6a33641e8bf0eedaa"
          }
        },
        "0098a541c98f4ed1abc17836d1669e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6af42cd8226c45a9842bfa70ff36f716",
            "placeholder": "​",
            "style": "IPY_MODEL_2090a4fac3654bd5a9485ccc52eaa18b",
            "value": "generation_config.json: 100%"
          }
        },
        "2067c584e4e1437bb02772796c6210cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92d7a08543ae41ec8ad7807226cc8ffb",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47accfc44a6e4330bed9e44dbc58a697",
            "value": 119
          }
        },
        "4742fe0813114583aa036c479959c0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_792e4f45bf974eecb8e39426e68322f6",
            "placeholder": "​",
            "style": "IPY_MODEL_40b8836d0a494acf9db286947f93d293",
            "value": " 119/119 [00:00&lt;00:00, 6.09kB/s]"
          }
        },
        "c0d41eeff73e4ec6a33641e8bf0eedaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af42cd8226c45a9842bfa70ff36f716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2090a4fac3654bd5a9485ccc52eaa18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92d7a08543ae41ec8ad7807226cc8ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47accfc44a6e4330bed9e44dbc58a697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "792e4f45bf974eecb8e39426e68322f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40b8836d0a494acf9db286947f93d293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The prompt used here concatenates design parameters with green practices and gives instruction. The response generated repeats certain patterns and does not mention all correct patterns followed"
      ],
      "metadata": {
        "id": "OuwCXcDONN90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import files\n",
        "\n",
        "# Upload CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the CSV file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "green_practices = pd.read_csv(file_name)\n",
        "\n",
        "# Display the columns in the CSV file to identify the correct column name\n",
        "print(green_practices.columns)\n",
        "\n",
        "\n",
        "green_practices_column = 'Green Practice'\n",
        "\n",
        "# Define design parameters\n",
        "design_parameters = [\n",
        "    \"The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\",\n",
        "    \"Memory usage is optimized by using memory-efficient data structures and algorithms.\",\n",
        "    \"All transmitted and stored data will be compressed using gzip.\",\n",
        "    \"Servers are located in regions closest to end-users to minimize latency.\",\n",
        "    \"Only sensitive data will be encrypted to reduce processing overhead.\"\n",
        "]\n",
        "\n",
        "# Create the prompt\n",
        "def create_prompt(design_params, green_practices, column_name):\n",
        "    prompt = \"Evaluate the following design parameters against the given green practices:\\n\\n\"\n",
        "    prompt += \"Design Parameters:\\n\"\n",
        "    for param in design_params:\n",
        "        prompt += f\"- {param}\\n\"\n",
        "    prompt += \"\\nGreen Practices:\\n\"\n",
        "    for practice in green_practices[column_name]:\n",
        "        prompt += f\"- {practice}\\n\"\n",
        "    prompt += \"\\nBased on the above information, indicate which green practices are being followed and which are not.\"\n",
        "    return prompt\n",
        "\n",
        "initial_prompt = create_prompt(design_parameters, green_practices, green_practices_column)\n",
        "print(initial_prompt)\n",
        "\n",
        "# Load a lightweight, open-source language model\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(prompt, max_length=600, max_new_tokens=150):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs.input_ids, max_length=max_length, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Generate the response\n",
        "response_text = generate_response(initial_prompt)\n",
        "print(response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rMp9jr5PEFxR",
        "outputId": "210ae790-b318-4b44-a5f0-b5f4db2831f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-91d1cf92-d5e3-46c0-b271-f4329aec9453\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-91d1cf92-d5e3-46c0-b271-f4329aec9453\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataGSF.csv to dataGSF (2).csv\n",
            "Index(['Category', 'Green Practice', 'Description'], dtype='object')\n",
            "Evaluate the following design parameters against the given green practices:\n",
            "\n",
            "Design Parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "\n",
            "Green Practices:\n",
            "- Cache static data\n",
            "- Choose the region that is closest to users\n",
            "- Compress stored data\n",
            "- Compress transmitted data\n",
            "- Containerize your workloads\n",
            "- Delete unused storage resources\n",
            "- Encrypt what is necessary\n",
            "- Evaluate other CPU architectures\n",
            "- Use a service mesh only if needed\n",
            "- Terminate TLS at border gateway\n",
            "- Implement stateless design\n",
            "- Match your service level objectives to business needs\n",
            "- Match utilization requirements of virtual machines (VMs)\n",
            "- Match utilization requirements with pre-configured servers\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Minimize the total number of deployed environments\n",
            "- Optimize storage utilization\n",
            "- Optimize average CPU utilization\n",
            "- Optimize impact on customer devices and equipment\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Remove unused assets\n",
            "- Scale down Kubernetes applications when not in use\n",
            "- Scale down applications when not in use\n",
            "- Scale infrastructure with user load\n",
            "- Scale Kubernetes workloads based on relevant demand metrics\n",
            "- Scale logical components independently\n",
            "- Scan for vulnerabilities\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use serverless cloud services\n",
            "- Optimize the size of AI/ML models\n",
            "- Use efficient file formats for AI/ML development\n",
            "- Run AI models at the edge\n",
            "- Select a more energy efficient AI/ML framework\n",
            "- Use energy efficient AI/ML models\n",
            "- Use sustainable regions for AI/ML training\n",
            "- Leverage pre-trained models and transfer learning for AI/ML development\n",
            "- Select the right hardware/VM instance types for AI/ML training\n",
            "- Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Based on the above information, indicate which green practices are being followed and which are not.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate the following design parameters against the given green practices:\n",
            "\n",
            "Design Parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "\n",
            "Green Practices:\n",
            "- Cache static data\n",
            "- Choose the region that is closest to users\n",
            "- Compress stored data\n",
            "- Compress transmitted data\n",
            "- Containerize your workloads\n",
            "- Delete unused storage resources\n",
            "- Encrypt what is necessary\n",
            "- Evaluate other CPU architectures\n",
            "- Use a service mesh only if needed\n",
            "- Terminate TLS at border gateway\n",
            "- Implement stateless design\n",
            "- Match your service level objectives to business needs\n",
            "- Match utilization requirements of virtual machines (VMs)\n",
            "- Match utilization requirements with pre-configured servers\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Minimize the total number of deployed environments\n",
            "- Optimize storage utilization\n",
            "- Optimize average CPU utilization\n",
            "- Optimize impact on customer devices and equipment\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Remove unused assets\n",
            "- Scale down Kubernetes applications when not in use\n",
            "- Scale down applications when not in use\n",
            "- Scale infrastructure with user load\n",
            "- Scale Kubernetes workloads based on relevant demand metrics\n",
            "- Scale logical components independently\n",
            "- Scan for vulnerabilities\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use serverless cloud services\n",
            "- Optimize the size of AI/ML models\n",
            "- Use efficient file formats for AI/ML development\n",
            "- Run AI models at the edge\n",
            "- Select a more energy efficient AI/ML framework\n",
            "- Use energy efficient AI/ML models\n",
            "- Use sustainable regions for AI/ML training\n",
            "- Leverage pre-trained models and transfer learning for AI/ML development\n",
            "- Select the right hardware/VM instance types for AI/ML training\n",
            "- Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Based on the above information, indicate which green practices are being followed and which are not.\n",
            "\n",
            "Green Practices:\n",
            "- Use a service mesh only if needed\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt is given more context but the response generated has the same issues as mentioned above."
      ],
      "metadata": {
        "id": "jNfFMKpVNsal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import files\n",
        "\n",
        "# Upload CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the CSV file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "green_practices = pd.read_csv(file_name)\n",
        "\n",
        "# Display the columns in the CSV file to identify the correct column name\n",
        "print(green_practices.columns)\n",
        "\n",
        "\n",
        "green_practices_column = 'Green Practice'\n",
        "\n",
        "# Define design parameters\n",
        "design_parameters = [\n",
        "    \"The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\",\n",
        "    \"Memory usage is optimized by using memory-efficient data structures and algorithms.\",\n",
        "    \"All transmitted and stored data will be compressed using gzip.\",\n",
        "    \"Servers are located in regions closest to end-users to minimize latency.\",\n",
        "    \"Only sensitive data will be encrypted to reduce processing overhead.\"\n",
        "]\n",
        "\n",
        "# Create the prompt\n",
        "def create_prompt(design_params, green_practices, column_name):\n",
        "    prompt = \"You are an expert in sustainable software design. Evaluate the following design parameters against the given green practices. Indicate which green practices are being followed and provide explanations for each. \"\n",
        "    prompt += \"Design Parameters:\\n\"\n",
        "    for param in design_params:\n",
        "        prompt += f\"- {param}\\n\"\n",
        "    prompt += \"\\nGreen Practices:\\n\"\n",
        "    for practice in green_practices[column_name]:\n",
        "        prompt += f\"- {practice}\\n\"\n",
        "    prompt += \"\\nBased on the above information, indicate which green practices are being followed and which are not.\"\n",
        "    return prompt\n",
        "\n",
        "initial_prompt = create_prompt(design_parameters, green_practices, green_practices_column)\n",
        "print(initial_prompt)\n",
        "\n",
        "# Load a lightweight, open-source language model\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(prompt, max_length=600, max_new_tokens=150):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs.input_ids, max_length=max_length, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Generate the response\n",
        "response_text = generate_response(initial_prompt)\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WVBzCZnyBKbG",
        "outputId": "907af54c-87eb-4235-fefa-b0bb521a35d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-658e01ec-7616-4295-99d1-cff907519b51\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-658e01ec-7616-4295-99d1-cff907519b51\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataGSF.csv to dataGSF (1).csv\n",
            "Index(['Category', 'Green Practice', 'Description', 'Unnamed: 3', 'Unnamed: 4',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Source'],\n",
            "      dtype='object')\n",
            "You are an expert in sustainable software design. Evaluate the following design parameters against the given green practices. Indicate which green practices are being followed and provide explanations for each. Design Parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "\n",
            "Green Practices:\n",
            "- Cache static data\n",
            "- Choose the region that is closest to users\n",
            "- Compress stored data\n",
            "- Compress transmitted data\n",
            "- Containerize your workloads\n",
            "- Delete unused storage resources\n",
            "- Encrypt what is necessary\n",
            "- Evaluate other CPU architectures\n",
            "- Use a service mesh only if needed\n",
            "- Terminate TLS at border gateway\n",
            "- Implement stateless design\n",
            "- Match your service level objectives to business needs\n",
            "- Match utilization requirements of virtual machines (VMs)\n",
            "- Match utilization requirements with pre-configured servers\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Minimize the total number of deployed environments\n",
            "- Optimize storage utilization\n",
            "- Optimize average CPU utilization\n",
            "- Optimize impact on customer devices and equipment\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Remove unused assets\n",
            "- Scale down Kubernetes applications when not in use\n",
            "- Scale down applications when not in use\n",
            "- Scale infrastructure with user load\n",
            "- Scale Kubernetes workloads based on relevant demand metrics\n",
            "- Scale logical components independently\n",
            "- Scan for vulnerabilities\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use serverless cloud services\n",
            "- Optimize the size of AI/ML models\n",
            "- Use efficient file formats for AI/ML development\n",
            "- Run AI models at the edge\n",
            "- Select a more energy efficient AI/ML framework\n",
            "- Use energy efficient AI/ML models\n",
            "- Use sustainable regions for AI/ML training\n",
            "- Leverage pre-trained models and transfer learning for AI/ML development\n",
            "- Select the right hardware/VM instance types for AI/ML training\n",
            "- Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Based on the above information, indicate which green practices are being followed and which are not.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an expert in sustainable software design. Evaluate the following design parameters against the given green practices. Indicate which green practices are being followed and provide explanations for each. Design Parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "\n",
            "Green Practices:\n",
            "- Cache static data\n",
            "- Choose the region that is closest to users\n",
            "- Compress stored data\n",
            "- Compress transmitted data\n",
            "- Containerize your workloads\n",
            "- Delete unused storage resources\n",
            "- Encrypt what is necessary\n",
            "- Evaluate other CPU architectures\n",
            "- Use a service mesh only if needed\n",
            "- Terminate TLS at border gateway\n",
            "- Implement stateless design\n",
            "- Match your service level objectives to business needs\n",
            "- Match utilization requirements of virtual machines (VMs)\n",
            "- Match utilization requirements with pre-configured servers\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Minimize the total number of deployed environments\n",
            "- Optimize storage utilization\n",
            "- Optimize average CPU utilization\n",
            "- Optimize impact on customer devices and equipment\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Remove unused assets\n",
            "- Scale down Kubernetes applications when not in use\n",
            "- Scale down applications when not in use\n",
            "- Scale infrastructure with user load\n",
            "- Scale Kubernetes workloads based on relevant demand metrics\n",
            "- Scale logical components independently\n",
            "- Scan for vulnerabilities\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use serverless cloud services\n",
            "- Optimize the size of AI/ML models\n",
            "- Use efficient file formats for AI/ML development\n",
            "- Run AI models at the edge\n",
            "- Select a more energy efficient AI/ML framework\n",
            "- Use energy efficient AI/ML models\n",
            "- Use sustainable regions for AI/ML training\n",
            "- Leverage pre-trained models and transfer learning for AI/ML development\n",
            "- Select the right hardware/VM instance types for AI/ML training\n",
            "- Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Based on the above information, indicate which green practices are being followed and which are not.\n",
            "\n",
            "Green Practices:\n",
            "- Use a service mesh only if needed\n",
            "- Use cloud native network security tools and controls\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Optimize peak CPU utilization\n",
            "- Use queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce transmitted data\n",
            "- Reduce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No response generated despite giving example in prompt."
      ],
      "metadata": {
        "id": "2ZqRAELaOKUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bl6devF1eg",
        "outputId": "c6731613-0d06-4382-98d8-a8c1cc671a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "NNF2UzaZGI4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the CSV file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "green_practices = pd.read_csv(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "UmPqlzz4GQ08",
        "outputId": "bc63ee9d-bea3-4a0f-cbbb-d51befae0c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36951767-7d37-46c3-bae2-18d857c6c307\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-36951767-7d37-46c3-bae2-18d857c6c307\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataGSF.csv to dataGSF.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the columns in the CSV file to identify the correct column name\n",
        "print(green_practices.columns)\n",
        "\n",
        "\n",
        "green_practices_column = 'Green Practice'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7wukjtVGW2L",
        "outputId": "8a55009e-b5f9-45e8-d841-9711ebdf284e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Category', 'Green Practice', 'Description', 'Unnamed: 3', 'Unnamed: 4',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Source'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define design parameters\n",
        "design_parameters = [\n",
        "    \"The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\",\n",
        "    \"Memory usage is optimized by using memory-efficient data structures and algorithms.\",\n",
        "    \"All transmitted and stored data will be compressed using gzip.\",\n",
        "    \"Servers are located in regions closest to end-users to minimize latency.\",\n",
        "    \"Only sensitive data will be encrypted to reduce processing overhead.\"\n",
        "]"
      ],
      "metadata": {
        "id": "rX49nln8GYPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(design_params, green_practices, column_name):\n",
        "    prompt = (\n",
        "        \"You are an expert in sustainable software design. Evaluate the following design parameters \"\n",
        "        \"against the given green practices. Indicate which green practices are being followed and provide \"\n",
        "        \"explanations for each. Ensure your responses are concise and relevant.\\n\\n\"\n",
        "        \"Example:\\n\"\n",
        "        \"Design Parameter: Optimize peak CPU utilization\\n\"\n",
        "        \"Green Practice: Minimize total number of deployed environments\\n\"\n",
        "        \"Explanation: Optimizing peak CPU utilization helps to minimize the number of environments needed, \"\n",
        "        \"reducing resource usage and environmental impact.\\n\\n\"\n",
        "        \"Now, evaluate the following design parameters:\\n\\n\"\n",
        "        \"Design Parameters:\\n\"\n",
        "    )\n",
        "    for param in design_params:\n",
        "        prompt += f\"- {param}\\n\"\n",
        "    prompt += \"\\nGreen Practices:\\n\"\n",
        "    for practice in green_practices[column_name]:\n",
        "        prompt += f\"- {practice}\\n\"\n",
        "    prompt += \"\\nProvide your evaluations below:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "\n",
        "initial_prompt = create_prompt(design_parameters, green_practices, green_practices_column)\n",
        "print(initial_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtqwRvg1GeaJ",
        "outputId": "4f7ed622-3c89-49cf-b1c3-eb1dde164d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an expert in sustainable software design. Evaluate the following design parameters against the given green practices. Indicate which green practices are being followed and provide explanations for each. Ensure your responses are concise and relevant.\n",
            "\n",
            "Now, evaluate the following design parameters:\n",
            "\n",
            "Design Parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "\n",
            "Green Practices:\n",
            "- Cache static data\n",
            "- Choose the region that is closest to users\n",
            "- Compress stored data\n",
            "- Compress transmitted data\n",
            "- Containerize your workloads\n",
            "- Delete unused storage resources\n",
            "- Encrypt what is necessary\n",
            "- Evaluate other CPU architectures\n",
            "- Use a service mesh only if needed\n",
            "- Terminate TLS at border gateway\n",
            "- Implement stateless design\n",
            "- Match your service level objectives to business needs\n",
            "- Match utilization requirements of virtual machines (VMs)\n",
            "- Match utilization requirements with pre-configured servers\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Minimize the total number of deployed environments\n",
            "- Optimize storage utilization\n",
            "- Optimize average CPU utilization\n",
            "- Optimize impact on customer devices and equipment\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Remove unused assets\n",
            "- Scale down Kubernetes applications when not in use\n",
            "- Scale down applications when not in use\n",
            "- Scale infrastructure with user load\n",
            "- Scale Kubernetes workloads based on relevant demand metrics\n",
            "- Scale logical components independently\n",
            "- Scan for vulnerabilities\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use serverless cloud services\n",
            "- Optimize the size of AI/ML models\n",
            "- Use efficient file formats for AI/ML development\n",
            "- Run AI models at the edge\n",
            "- Select a more energy efficient AI/ML framework\n",
            "- Use energy efficient AI/ML models\n",
            "- Use sustainable regions for AI/ML training\n",
            "- Leverage pre-trained models and transfer learning for AI/ML development\n",
            "- Select the right hardware/VM instance types for AI/ML training\n",
            "- Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Provide your evaluations below:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a lightweight, open-source language model\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "rVzmNX3TGhN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "cc62751944af483180db26fd91bee0cf",
            "a5d2a524b16646eab1152cbeade867b7",
            "f61e2d13548745159609cbbccce24795",
            "d981945257f74ab6a4ff8a3392e69fb6",
            "a39da6d399874e7f80682fdfebbe5caa",
            "ddca93754a044ca5be344ba9e5ba6e0b",
            "0bf1092fcec4453b99a51801c04e98d4",
            "463838c29f234a8e86dce89d95436c30",
            "6ea3b9f9bca2457190f2083f6fee27c8",
            "64466599b1d542e69e61321d5fa55fdb",
            "65717a2bd44d498bb1fc44af10b1f73d",
            "3a297524ae364f2b808392e0a183baa1",
            "6e84b730b4bd479faba620179b5c1e08",
            "14538039b8bd479ebc6bf00d1b45daf8",
            "6ae1d13e53c443d79c51f9f2521fcbab",
            "c25a73fd05534780a91474fd3331a3c7",
            "0774f00841b14438ac51a13acf3c6bbc",
            "15021e72b8f447b5ad7c7f3721b6a6be",
            "62c3c3ada016494b834778fd9e79fdd9",
            "cbfc0b38f20048a6861d8902b08bef33",
            "07a2f6dc5b344f8ab431b4ce57fb9d0f",
            "dddea27043aa4364920bb248e89f62c3",
            "14a0d540008a4b0cb2a83c170d04763e",
            "e85729e232a740b8ba74fb873d8c65cb",
            "e7035bd26da543608daeeac82671687b",
            "558239a567024b5fb3967d66e93bc618",
            "7ed1593998e14ed1be7a6229200d5a75",
            "52042a64dced4d8895346651d0b7f671",
            "827d543ae0be4316bee9e54fb38a462f",
            "c954bb50db444b4c8536da694310021a",
            "7f0186f00e254cbcbb160abf271f073c",
            "66d5a66494be4a9bbe6b586a03918be7",
            "ae7865408e6847aabd64217ceb726799",
            "b721cc7c167a4404a46ca02d211b8a23",
            "3cdb40c641eb40d881d838fa45e59e6a",
            "7ee406e9c1544faa9a415d1e328497cb",
            "239b75bbcec9427fb7a172c7663f5124",
            "5319fa75e0454e77bbd04aaa4fadd57e",
            "8b74d89b020e40ffb03354e0340a4bef",
            "489a2051dbc94b47a89a14fd0d68c30c",
            "f7681586e480468999db79557ce93832",
            "93c044674a5744fbb6de37092221510d",
            "73f602c68f384c828325f98be21aa251",
            "44a5a3176e48464b9e6f081c1a18e4de",
            "4774ae8e5ff44e6ca70eb99a94e47581",
            "c3c77dfa1bd34deb9e9f9dc4e48e20c0",
            "88d6b1bf4a644c3886964517de6ef623",
            "3bf1afb7df3346938f820b38af52c6f2",
            "e970050d95244554b7224f960134cd89",
            "e3c70dad6c9a488ebf102bb84fe93c05",
            "f3291db1cf4848abb982a72897f83cfc",
            "6d387e0751c5447891bc686f5a40508b",
            "878413f7d065459c9472d5fdbf34e7e5",
            "23cd18f72c194dcc8b9301f5bab81479",
            "8a73e089167049748a7567fda1798aa5",
            "831c727e8b7548b895b25c2998d4de3f",
            "9086bbd4c6334540aa43ba93a74a8d60",
            "e1b0e6b0d29a47d5879f158e4361e74b",
            "6085bc5ce4344e048649e2a768e12395",
            "8c1206d775e74a4f94bb13272055f0d0",
            "c6454aff3d9048a285a0a58476066c3c",
            "929ec3ef185e4443b86ef3937b30413c",
            "e6916530012243299f26698a7ffd7ec0",
            "d08d74cbce444abda419d72f16376e10",
            "d69b6a586cf741808b515c637a5debaf",
            "87b9ab37968a4a2b8351f0324170e8cf",
            "81e42e1da1fb4b3aa52a9ef0b089a786",
            "d2eaa79537054cf393fe9f6349e36ee7",
            "0d07f0376846415f809e023cb2ecd616",
            "857f8f82c6db4db7857b0f7973f2825a",
            "2358a33ca1cf4c7ca169eca88794807a",
            "31b7d97211e849fe975685d202ea5d8c",
            "3689978b85604049b6b33f817d5b54cc",
            "104e50c49b764a879efd7a2925991df6",
            "e6ac1f8f67ea4357ac1e23c5caabf79b",
            "403bbce82cc14338a46688d04f2b09ea",
            "da128c30e45e4507943d59dc57aa2248",
            "74fd9fd0083b43ffb7aede27d512c11e",
            "0098a541c98f4ed1abc17836d1669e9a",
            "2067c584e4e1437bb02772796c6210cf",
            "4742fe0813114583aa036c479959c0b9",
            "c0d41eeff73e4ec6a33641e8bf0eedaa",
            "6af42cd8226c45a9842bfa70ff36f716",
            "2090a4fac3654bd5a9485ccc52eaa18b",
            "92d7a08543ae41ec8ad7807226cc8ffb",
            "47accfc44a6e4330bed9e44dbc58a697",
            "792e4f45bf974eecb8e39426e68322f6",
            "40b8836d0a494acf9db286947f93d293"
          ]
        },
        "outputId": "1d854835-8dd1-4c7a-f061-8c4f5af1a642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc62751944af483180db26fd91bee0cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a297524ae364f2b808392e0a183baa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14a0d540008a4b0cb2a83c170d04763e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b721cc7c167a4404a46ca02d211b8a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4774ae8e5ff44e6ca70eb99a94e47581"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "831c727e8b7548b895b25c2998d4de3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81e42e1da1fb4b3aa52a9ef0b089a786"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74fd9fd0083b43ffb7aede27d512c11e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response from the model\n",
        "def generate_response(prompt, max_length=800, max_new_tokens=150, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "wVsQgGQ8Gmao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the response\n",
        "response_text = generate_response(initial_prompt)\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SStvRAMGGn5U",
        "outputId": "a9c91917-274d-4d66-ec46-a46541613dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an expert in sustainable software design. Evaluate the following design parameters against the given green practices. Indicate which green practices are being followed and provide explanations for each. Ensure your responses are concise and relevant.\n",
            "\n",
            "Now, evaluate the following design parameters:\n",
            "\n",
            "Design Parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "\n",
            "Green Practices:\n",
            "- Cache static data\n",
            "- Choose the region that is closest to users\n",
            "- Compress stored data\n",
            "- Compress transmitted data\n",
            "- Containerize your workloads\n",
            "- Delete unused storage resources\n",
            "- Encrypt what is necessary\n",
            "- Evaluate other CPU architectures\n",
            "- Use a service mesh only if needed\n",
            "- Terminate TLS at border gateway\n",
            "- Implement stateless design\n",
            "- Match your service level objectives to business needs\n",
            "- Match utilization requirements of virtual machines (VMs)\n",
            "- Match utilization requirements with pre-configured servers\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use cloud native processor VMs\n",
            "- Use serverless cloud services\n",
            "- Minimize total number of deployed environments\n",
            "- Minimize the total number of deployed environments\n",
            "- Optimize storage utilization\n",
            "- Optimize average CPU utilization\n",
            "- Optimize impact on customer devices and equipment\n",
            "- Optimize peak CPU utilization\n",
            "- Queue non-urgent processing requests\n",
            "- Reduce transmitted data\n",
            "- Remove unused assets\n",
            "- Scale down Kubernetes applications when not in use\n",
            "- Scale down applications when not in use\n",
            "- Scale infrastructure with user load\n",
            "- Scale Kubernetes workloads based on relevant demand metrics\n",
            "- Scale logical components independently\n",
            "- Scan for vulnerabilities\n",
            "- Set storage retention policies\n",
            "- Shed lower priority traffic\n",
            "- Time-shift Kubernetes cron jobs\n",
            "- Use asynchronous network calls instead of synchronous\n",
            "- Use circuit breaker patterns\n",
            "- Use cloud native network security tools and controls\n",
            "- Use DDoS protection\n",
            "- Use serverless cloud services\n",
            "- Optimize the size of AI/ML models\n",
            "- Use efficient file formats for AI/ML development\n",
            "- Run AI models at the edge\n",
            "- Select a more energy efficient AI/ML framework\n",
            "- Use energy efficient AI/ML models\n",
            "- Use sustainable regions for AI/ML training\n",
            "- Leverage pre-trained models and transfer learning for AI/ML development\n",
            "- Select the right hardware/VM instance types for AI/ML training\n",
            "- Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Provide your evaluations below:\n",
            "\n",
            "- Evaluate the following design parameters:\n",
            "- The application will scale CPU usage based on real-time demand, with peak utilization not exceeding 80%.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- Memory usage is optimized by using memory-efficient data structures and algorithms.\n",
            "- All transmitted and stored data will be compressed using gzip.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "- Servers are located in regions closest to end-users to minimize latency.\n",
            "- Only sensitive data will be encrypted to reduce processing overhead.\n",
            "- Servers are located in regions closest to end-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The response diverged into an unrelated discussion about machine learning steps, which is not aligned with the task of evaluating design parameters against green practices."
      ],
      "metadata": {
        "id": "4XrbS2MlOVqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import files\n",
        "\n",
        "# Upload CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the CSV file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "green_practices = pd.read_csv(file_name)\n",
        "\n",
        "# Display the columns in the CSV file to identify the correct column name\n",
        "print(green_practices.columns)\n",
        "\n",
        "\n",
        "green_practices_column = 'Green Practice'\n",
        "\n",
        "# Define design parameters for evaluation\n",
        "design_parameters = [\n",
        "    \"The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\",\n",
        "    \"Memory usage is optimized using memory-efficient data structures and algorithms.\",\n",
        "    \"Data is compressed using gzip for both transmission and storage.\",\n",
        "    \"Servers are positioned in regions near end-users to reduce latency.\",\n",
        "    \"Only sensitive data is encrypted to minimize processing overhead.\"\n",
        "]\n",
        "\n",
        "# Function to create a more structured and clear prompt\n",
        "def create_prompt(design_params, green_practices, column_name):\n",
        "    prompt = (\n",
        "        \"You are a sustainable software design expert tasked with evaluating the following design parameters \"\n",
        "        \"against a set of green software practices. For each design parameter, indicate which green practices \"\n",
        "        \"are being followed and provide a brief explanation. \\n\\n\"\n",
        "    )\n",
        "    prompt += \"### Design Parameters:\\n\"\n",
        "    for idx, param in enumerate(design_params, 1):\n",
        "        prompt += f\"{idx}. {param}\\n\"\n",
        "\n",
        "    prompt += \"\\n### Green Software Practices:\\n\"\n",
        "    for idx, practice in enumerate(green_practices[column_name], 1):\n",
        "        prompt += f\"{idx}. {practice}\\n\"\n",
        "\n",
        "    prompt += (\n",
        "        \"\\n### Task:\\n\"\n",
        "        \"For each design parameter, list the green software practices being followed and explain why they match. \"\n",
        "        \"If none apply, indicate that as well.\\n\"\n",
        "    )\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Create the initial prompt using the structured format\n",
        "initial_prompt = create_prompt(design_parameters, green_practices, green_practices_column)\n",
        "print(\"Generated Prompt: \\n\", initial_prompt)\n",
        "\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate the response from the model\n",
        "def generate_response(prompt, max_length=800, max_new_tokens=300):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Generate the response\n",
        "response_text = generate_response(initial_prompt)\n",
        "print(\"Model's Response: \\n\", response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zrckF_PwDStT",
        "outputId": "3be01478-0067-4f0d-9897-f020e654b06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2e296b6c-a4c6-4510-a224-fe3661a88f5e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2e296b6c-a4c6-4510-a224-fe3661a88f5e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataGSF.csv to dataGSF (2).csv\n",
            "Index(['Category', 'Green Practice', 'Description', 'Unnamed: 3', 'Unnamed: 4',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Source'],\n",
            "      dtype='object')\n",
            "Generated Prompt: \n",
            " You are a sustainable software design expert tasked with evaluating the following design parameters against a set of green software practices. For each design parameter, indicate which green practices are being followed and provide a brief explanation. \n",
            "\n",
            "### Design Parameters:\n",
            "1. The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "2. Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "3. Data is compressed using gzip for both transmission and storage.\n",
            "4. Servers are positioned in regions near end-users to reduce latency.\n",
            "5. Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Software Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "### Task:\n",
            "For each design parameter, list the green software practices being followed and explain why they match. If none apply, indicate that as well.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Response: \n",
            " You are a sustainable software design expert tasked with evaluating the following design parameters against a set of green software practices. For each design parameter, indicate which green practices are being followed and provide a brief explanation. \n",
            "\n",
            "### Design Parameters:\n",
            "1. The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "2. Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "3. Data is compressed using gzip for both transmission and storage.\n",
            "4. Servers are positioned in regions near end-users to reduce latency.\n",
            "5. Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Software Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "### Task:\n",
            "For each design parameter, list the green software practices being followed and explain why they match. If none apply, indicate that as well.\n",
            "1) **Use a local AI/VM model**\n",
            "2) **Set a new local machine**\n",
            "3) **Enable your local machine to run a machine-learning algorithm**\n",
            "4) **Create a new machine** in your local cluster\n",
            "5) **Apply an AI/CVM model to a machine**.\n",
            "6) **Do a machine learning algorithm**. This is the most important step.\n",
            "7) **Assign a new Machine**. For this step, you can add more data, such as a new CPU, a new storage resource, a serverless environment, and a new data structure.\n",
            "8) **Run a machine training algorithm** for this step. You can use this step as a starting point to run the machine training, but it will be very useful for creating machine-training and machine-learning algorithms. The algorithm should be based on your machine training data.\n",
            "9) **Build a new model**. In this step you choose a new instance of your machine. At the end of this step (the part where you decide to run this step), you can start running the machine-time learning algorithm.\n",
            "10) **Read a machine data structure**. The data structure should have the following properties:\n",
            "\n",
            "  * It should be a single column, or a vector of data rows.\n",
            "  * The format should be as follows: xlsx, xls, xlsm, xllsm.\n",
            "11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model generated a response with a lot of redundancy and irrelevant information."
      ],
      "metadata": {
        "id": "W2IEbsmCPOU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import files\n",
        "\n",
        "# Upload CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the CSV file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "green_practices = pd.read_csv(file_name)\n",
        "\n",
        "# Display the columns in the CSV file to identify the correct column name\n",
        "print(green_practices.columns)\n",
        "\n",
        "\n",
        "green_practices_column = 'Green Practice'\n",
        "\n",
        "# Define design parameters for evaluation\n",
        "design_parameters = [\n",
        "    \"The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\",\n",
        "    \"Memory usage is optimized using memory-efficient data structures and algorithms.\",\n",
        "    \"Data is compressed using gzip for both transmission and storage.\",\n",
        "    \"Servers are positioned in regions near end-users to reduce latency.\",\n",
        "    \"Only sensitive data is encrypted to minimize processing overhead.\"\n",
        "]\n",
        "\n",
        "# Function to create a more structured and direct prompt\n",
        "def create_prompt(design_params, green_practices, column_name):\n",
        "    prompt = (\n",
        "        \"You are a sustainable software design expert. Evaluate the following design parameters against a list of green practices. \"\n",
        "        \"For each design parameter, identify which green practices it follows, and explain why. Use concise language.\\n\\n\"\n",
        "    )\n",
        "\n",
        "    prompt += \"### Example Format:\\n\"\n",
        "    prompt += \"Design Parameter: <Parameter>\\nGreen Practices: <Practice 1>, <Practice 2>\\nExplanation: <Brief explanation>\\n\\n\"\n",
        "\n",
        "    prompt += \"### Design Parameters and Green Practices:\\n\"\n",
        "    for idx, param in enumerate(design_params, 1):\n",
        "        prompt += f\"{idx}. Design Parameter: {param}\\n\"\n",
        "\n",
        "    prompt += \"\\n### Green Practices:\\n\"\n",
        "    for idx, practice in enumerate(green_practices[column_name], 1):\n",
        "        prompt += f\"{idx}. {practice}\\n\"\n",
        "\n",
        "    prompt += \"\\nNow, for each design parameter, list which green practices are followed and explain why.\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Create the initial prompt using the structured format\n",
        "initial_prompt = create_prompt(design_parameters, green_practices, green_practices_column)\n",
        "print(\"Generated Prompt: \\n\", initial_prompt)\n",
        "\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate the response from the model\n",
        "def generate_response(prompt, max_length=800, max_new_tokens=300):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Generate the response\n",
        "response_text = generate_response(initial_prompt)\n",
        "print(\"Model's Response: \\n\", response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "glOLi6JIEf2T",
        "outputId": "e28138f8-5faf-4ee6-8707-cb20edf6e0df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26145f6e-d9dc-487e-b981-700b1b5b1bf2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26145f6e-d9dc-487e-b981-700b1b5b1bf2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataGSF.csv to dataGSF (4).csv\n",
            "Index(['Category', 'Green Practice', 'Description', 'Unnamed: 3', 'Unnamed: 4',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Source'],\n",
            "      dtype='object')\n",
            "Generated Prompt: \n",
            " You are a sustainable software design expert. Evaluate the following design parameters against a list of green practices. For each design parameter, identify which green practices it follows, and explain why. Use concise language.\n",
            "\n",
            "### Example Format:\n",
            "Design Parameter: <Parameter>\n",
            "Green Practices: <Practice 1>, <Practice 2>\n",
            "Explanation: <Brief explanation>\n",
            "\n",
            "### Design Parameters and Green Practices:\n",
            "1. Design Parameter: The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "2. Design Parameter: Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "3. Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "4. Design Parameter: Servers are positioned in regions near end-users to reduce latency.\n",
            "5. Design Parameter: Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Now, for each design parameter, list which green practices are followed and explain why.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Response: \n",
            " You are a sustainable software design expert. Evaluate the following design parameters against a list of green practices. For each design parameter, identify which green practices it follows, and explain why. Use concise language.\n",
            "\n",
            "### Example Format:\n",
            "Design Parameter: <Parameter>\n",
            "Green Practices: <Practice 1>, <Practice 2>\n",
            "Explanation: <Brief explanation>\n",
            "\n",
            "### Design Parameters and Green Practices:\n",
            "1. Design Parameter: The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "2. Design Parameter: Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "3. Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "4. Design Parameter: Servers are positioned in regions near end-users to reduce latency.\n",
            "5. Design Parameter: Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "Now, for each design parameter, list which green practices are followed and explain why.\n",
            "For example:\n",
            "\n",
            "1 - Cache static content in RAM.\n",
            "2 - Choose the most efficient data structure.\n",
            "In this example, if you use a file system (such as a block storage device or a file), it is not efficient if you have to keep a list in memory.\n",
            "You can think of this as a design parameter where a developer would need to consider all the possible designs for the storage architecture.\n",
            "The file system is used to store data that is important for the application. The developer must think about the entire file system architecture, and determine the best way to store this important data.\n",
            "If you are using a database, you have an application that is sensitive to the storage, and need to be as efficient as possible to store sensitive data in a database.\n",
            "Data is stored in a file that is shared across machines. You can think about this as the same type of data structure, but where data is stored separately.\n",
            "To save space, you want to have a file type that is efficient so that you can store a lot of data efficiently.\n",
            "As a developer, you would need a lot to do your job, and you need to think about all the ways you can save space.\n",
            "So, it is a design parameters where a developers job is to consider how the data can be stored.\n",
            "Another example: In an image processing application, if the image is stored on a disk, the disk is not optimized, and can take a lot more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model's response once again drifted into irrelevant and technical details"
      ],
      "metadata": {
        "id": "z-kKAGs1Ph2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refined function to generate the prompt\n",
        "def create_refined_prompt(design_params, green_practices, column_name):\n",
        "    prompt = (\n",
        "        \"You are a sustainable software design expert. Match the design parameters below with the relevant green practices from the provided list. \"\n",
        "        \"For each design parameter, list the green practices it follows and briefly explain why.\\n\\n\"\n",
        "        \"### Example:\\n\"\n",
        "        \"Design Parameter: Data is compressed using gzip for both transmission and storage.\\n\"\n",
        "        \"Green Practices: Compress stored data, Compress transmitted data\\n\"\n",
        "        \"Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\\n\\n\"\n",
        "        \"### Design Parameters:\\n\"\n",
        "    )\n",
        "\n",
        "    for idx, param in enumerate(design_params, 1):\n",
        "        prompt += f\"{idx}. {param}\\n\"\n",
        "\n",
        "    prompt += \"\\n### Green Practices:\\n\"\n",
        "    for idx, practice in enumerate(green_practices[column_name], 1):\n",
        "        prompt += f\"{idx}. {practice}\\n\"\n",
        "\n",
        "    prompt += \"\\nFor each design parameter, list the relevant green practices and provide brief explanations.\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Create the refined prompt\n",
        "refined_prompt = create_refined_prompt(design_parameters, green_practices, green_practices_column)\n",
        "print(\"Refined Prompt: \\n\", refined_prompt)\n",
        "\n",
        "# Function to generate the response from the model\n",
        "def generate_refined_response(prompt, max_length=800, max_new_tokens=300):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Generate the refined response\n",
        "refined_response_text = generate_refined_response(refined_prompt)\n",
        "print(\"Refined Model's Response: \\n\", refined_response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iVBCe_yIiQK",
        "outputId": "498a84dd-394d-4622-a94f-5e8179b11bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Prompt: \n",
            " You are a sustainable software design expert. Match the design parameters below with the relevant green practices from the provided list. For each design parameter, list the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameters:\n",
            "1. The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "2. Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "3. Data is compressed using gzip for both transmission and storage.\n",
            "4. Servers are positioned in regions near end-users to reduce latency.\n",
            "5. Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "For each design parameter, list the relevant green practices and provide brief explanations.\n",
            "Refined Model's Response: \n",
            " You are a sustainable software design expert. Match the design parameters below with the relevant green practices from the provided list. For each design parameter, list the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameters:\n",
            "1. The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "2. Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "3. Data is compressed using gzip for both transmission and storage.\n",
            "4. Servers are positioned in regions near end-users to reduce latency.\n",
            "5. Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "For each design parameter, list the relevant green practices and provide brief explanations.\n",
            "\n",
            "\n",
            "## Design Parameters\n",
            "\n",
            "## Description\n",
            "\n",
            "Design parameters are the factors that are used to define the overall design. Some parameters are considered more important than others. For example, for an industrial or scientific application, the design is usually based on some parameters. These include:\n",
            "\n",
            "* **CPU** CPU needs for the application.\n",
            "*    **CPU usage** Current peak CPU usage for the environment.\n",
            "  *   The following values are considered for a typical KuberNETS design:\n",
            "   * **CPU utilization** Average CPU utilization for the entire environment. This variable indicates the average CPU usage over the entire deployment period.\n",
            "\t* **Scalable** CPU utilization levels. Scalable means the CPU utilization can be changed based on the actual demand, e.g., peak demand.\n",
            "```yaml\n",
            "  Scalable:\n",
            "\t- CPU_USAGE_PEAK:\n",
            "```.\n",
            "\n",
            "These values are important because they determine the overall efficiency of the system. Scalability refers to how the CPU usage can be dynamically adjusted based on actual demand. The following example illustrates how to adjust the CPU demand for the system:\n",
            ">\n",
            ">   kube::service_account::accountname:\n",
            "<br>\n",
            "\t   - CPU_USE_PEAKING_FOR_USER:\n",
            "</br>\n",
            "\n",
            "   CPU_PEAS_NUM_PEOPLE:\n",
            ".\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to handle each design parameter separately\n",
        "def create_individual_prompt(design_param, green_practices, column_name):\n",
        "    prompt = (\n",
        "        \"You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. \"\n",
        "        \"List the green practices it follows and briefly explain why.\\n\\n\"\n",
        "        \"### Example:\\n\"\n",
        "        \"Design Parameter: Data is compressed using gzip for both transmission and storage.\\n\"\n",
        "        \"Green Practices: Compress stored data, Compress transmitted data\\n\"\n",
        "        \"Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\\n\\n\"\n",
        "        f\"### Design Parameter:\\n{design_param}\\n\\n\"\n",
        "        \"### Green Practices:\\n\"\n",
        "    )\n",
        "\n",
        "    for idx, practice in enumerate(green_practices[column_name], 1):\n",
        "        prompt += f\"{idx}. {practice}\\n\"\n",
        "\n",
        "    prompt += \"\\nList the relevant green practices and explain why.\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Now we loop through each design parameter and generate a response for each\n",
        "for design_param in design_parameters:\n",
        "    individual_prompt = create_individual_prompt(design_param, green_practices, green_practices_column)\n",
        "    print(f\"Prompt for: {design_param}\")\n",
        "    print(individual_prompt)\n",
        "\n",
        "    # Generate response for each individual prompt\n",
        "    response = generate_refined_response(individual_prompt)\n",
        "    print(f\"Response for: {design_param}\")\n",
        "    print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3JPyuHfvKZF3",
        "outputId": "abf817f0-06f7-4261-dade-9c40aced4346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt for: The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for: The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "The application scales CPU usage based on real-time demand, keeping peak utilization under 80%.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n",
            "\n",
            "\n",
            "### List the relevant Green Practices\n",
            "\n",
            "\n",
            "1 - Containerize Your Workloads\n",
            "2 - Delete Unused Storage Resources\n",
            "3 - Compress Stored Data\n",
            "4 - Compression\n",
            "5 - Compressing\n",
            "6 - Containerization\n",
            "7 - Compressed\n",
            "8 - Distributed\n",
            "9 - Compound\n",
            "10 - Compressor\n",
            "11 - Compressive\n",
            "12 - Compute\n",
            "13 - Compile\n",
            "14 - Compiling\n",
            "15 - Comparing\n",
            "16 - Compiler\n",
            "17 - Compilation\n",
            "18 - C Compiler (C and C++)\n",
            "19 - Compiled\n",
            "20 - Compilable\n",
            "21 - Compiles\n",
            "22 - Compilers\n",
            "23 - Compartmentalized\n",
            "24 - Compassionate\n",
            "25 - Compatability\n",
            "26 - Compromise\n",
            "27 - Competing\n",
            "28 - Comprehension\n",
            "29 - Comprehend\n",
            "30 - Compendium\n",
            "31 - Comprehensive\n",
            "32 - Compenced\n",
            "33 - Compent\n",
            "34 - Compend\n",
            "35 - Compending\n",
            "36 - Compensate\n",
            "37 - Compen\n",
            "38 - Compensable\n",
            "39 - Compense\n",
            "40 - Compensed\n",
            "41 - Compencing\n",
            "42 - Compenser\n",
            "43 - Compel\n",
            "44 - Compelle\n",
            "45 - Compella\n",
            "46 - Compelt\n",
            "47 - Compela\n",
            "48 - Compelin\n",
            "49 - Compell\n",
            "50 - Compelled\n",
            "51 - Compels\n",
            "52 - Compelli\n",
            "53 - Compellen\n",
            "54 -\n",
            "Prompt for: Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for: Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Memory usage is optimized using memory-efficient data structures and algorithms.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n",
            "\n",
            "\n",
            "\n",
            "## Reference\n",
            "[![GoDoc](https://godoc.org/github.com/hashicorp/terraform/api?status.svg)](https://github.blog/2020-02-28-go-terraforming-green-practices/)\n",
            "\n",
            "[GoTLD](https: //golang.org/)\n",
            "\n",
            "Prompt for: Data is compressed using gzip for both transmission and storage.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Data is compressed using gzip for both transmission and storage.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for: Data is compressed using gzip for both transmission and storage.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Data is compressed using gzip for both transmission and storage.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why. Feel free to share your expertise and experiences in the comments below.\n",
            "{% include_sub_folder content='_shared/green-practices.md' %}\n",
            "\n",
            "\n",
            "Prompt for: Servers are positioned in regions near end-users to reduce latency.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Servers are positioned in regions near end-users to reduce latency.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=300) and `max_length`(=800) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response for: Servers are positioned in regions near end-users to reduce latency.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Servers are positioned in regions near end-users to reduce latency.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n",
            "\n",
            "\n",
            "### List the Green Practices\n",
            "* [ ] Use server-side storage for AI and ML analytics\n",
            "* Use server side storage for development\n",
            "* Do not use cloud storage for analytics\n",
            "\n",
            "*\n",
            "\n",
            "\n",
            "*\n",
            "\n",
            "## Design Parameters\n",
            "\n",
            "**Note:** The green practices have been categorized into different categories, such as cloud, storage (private and public), compute, and networking and networking\n",
            "\n",
            "|   \n",
            "---|---\n",
            "\n",
            "The following table provides the list of the green parameters that can be used to help you design your architecture. Refer to the following table to identify the green parameter type, followed by the relevant parameters. You can use the provided table to define a green parameter and explain how it can match the green practice.\n",
            "\n",
            "\n",
            "\n",
            "| Parameter | Description |\n",
            "| --- | --- |\n",
            "\n",
            "#### Cloud Storage\n",
            "\n",
            "Cloud storage can be a single cloud provider or it can be multiple clouds. You may consider using multiple cloud providers for storage. For example, Amazon, Google, or Microsoft Cloud.\n",
            "A cloud storage provider may also integrate with the Amazon Simple Storage Service (Amazon S3) to provide secure storage. A storage provider can also use the Amazon Glacier for storage, which is not a public cloud storage service.\n",
            "There are no specific green practices or parameters that make it mandatory to use cloud services, except using multiple storage providers as explained above.\n",
            "#### Private Cloud Storage (Private)\n",
            "\n",
            "A private cloud storage can also\n",
            "Prompt for: Only sensitive data is encrypted to minimize processing overhead.\n",
            "You are a sustainable software design expert. Match the design parameter below with the relevant green practices from the provided list. List the green practices it follows and briefly explain why.\n",
            "\n",
            "### Example:\n",
            "Design Parameter: Data is compressed using gzip for both transmission and storage.\n",
            "Green Practices: Compress stored data, Compress transmitted data\n",
            "Explanation: This design parameter matches the green practices of compressing both stored and transmitted data, which helps reduce storage space and bandwidth usage.\n",
            "\n",
            "### Design Parameter:\n",
            "Only sensitive data is encrypted to minimize processing overhead.\n",
            "\n",
            "### Green Practices:\n",
            "1. Cache static data\n",
            "2. Choose the region that is closest to users\n",
            "3. Compress stored data\n",
            "4. Compress transmitted data\n",
            "5. Containerize your workloads\n",
            "6. Delete unused storage resources\n",
            "7. Encrypt what is necessary\n",
            "8. Evaluate other CPU architectures\n",
            "9. Use a service mesh only if needed\n",
            "10. Terminate TLS at border gateway\n",
            "11. Implement stateless design\n",
            "12. Match your service level objectives to business needs\n",
            "13. Match utilization requirements of virtual machines (VMs)\n",
            "14. Match utilization requirements with pre-configured servers\n",
            "15. Set storage retention policies\n",
            "16. Shed lower priority traffic\n",
            "17. Time-shift Kubernetes cron jobs\n",
            "18. Use asynchronous network calls instead of synchronous\n",
            "19. Use circuit breaker patterns\n",
            "20. Use cloud native network security tools and controls\n",
            "21. Use DDoS protection\n",
            "22. Use cloud native processor VMs\n",
            "23. Use serverless cloud services\n",
            "24. Minimize total number of deployed environments\n",
            "25. Minimize the total number of deployed environments\n",
            "26. Optimize storage utilization\n",
            "27. Optimize average CPU utilization\n",
            "28. Optimize impact on customer devices and equipment\n",
            "29. Optimize peak CPU utilization\n",
            "30. Queue non-urgent processing requests\n",
            "31. Reduce transmitted data\n",
            "32. Remove unused assets\n",
            "33. Scale down Kubernetes applications when not in use\n",
            "34. Scale down applications when not in use\n",
            "35. Scale infrastructure with user load\n",
            "36. Scale Kubernetes workloads based on relevant demand metrics\n",
            "37. Scale logical components independently\n",
            "38. Scan for vulnerabilities\n",
            "39. Set storage retention policies\n",
            "40. Shed lower priority traffic\n",
            "41. Time-shift Kubernetes cron jobs\n",
            "42. Use asynchronous network calls instead of synchronous\n",
            "43. Use circuit breaker patterns\n",
            "44. Use cloud native network security tools and controls\n",
            "45. Use DDoS protection\n",
            "46. Use serverless cloud services\n",
            "47. Optimize the size of AI/ML models\n",
            "48. Use efficient file formats for AI/ML development\n",
            "49. Run AI models at the edge\n",
            "50. Select a more energy efficient AI/ML framework\n",
            "51. Use energy efficient AI/ML models\n",
            "52. Use sustainable regions for AI/ML training\n",
            "53. Leverage pre-trained models and transfer learning for AI/ML development\n",
            "54. Select the right hardware/VM instance types for AI/ML training\n",
            "55. Adopt serverless architecture for AI/ML workload processes\n",
            "\n",
            "List the relevant green practices and explain why.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f7be9393919d>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Generate response for each individual prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_refined_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response for: {design_param}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-db0f225eb696>\u001b[0m in \u001b[0;36mgenerate_refined_response\u001b[0;34m(prompt, max_length, max_new_tokens)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_refined_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2025\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2982\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynced_gpus\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 )\n\u001b[1;32m    731\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    733\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}